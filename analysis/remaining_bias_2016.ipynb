{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases  in Word Embeddings But do not Remove Them\n",
    "\n",
    "For a detailed explanation of the experiments in this notebook, see:\n",
    "[paper](https://arxiv.org/pdf/1903.03862.pdf \"Lipstick on a Pig paper\")\n",
    "\n",
    "This notebook uses the debiased embeddings described in Bolukbasi et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print('loading ...')? (3472543045.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    print 'loading ...'\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('loading ...')?\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def load_embeddings_from_np(filename):\n",
    "    print 'loading ...'\n",
    "    with codecs.open(filename + '.vocab', 'r', 'utf-8') as f_embed:\n",
    "        vocab = [line.strip() for line in f_embed]\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.load(filename + '.wv.npy')\n",
    "\n",
    "    return vocab, wv, w2i\n",
    "\n",
    "\n",
    "def normalize(wv):\n",
    "    \n",
    "    # normalize vectors\n",
    "    norms = np.apply_along_axis(LA.norm, 1, wv)\n",
    "    wv = wv / norms[:, np.newaxis]\n",
    "    return wv\n",
    "\n",
    "\n",
    "def load_and_normalize(space, filename, vocab, wv, w2i):\n",
    "    vocab_muse, wv_muse, w2i_muse = load_embeddings_from_np(filename)\n",
    "    wv_muse = normalize(wv_muse)\n",
    "    vocab[space] = vocab_muse \n",
    "    wv[space] = wv_muse\n",
    "    w2i[space] = w2i_muse\n",
    "    print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "wv = {}\n",
    "w2i = {}\n",
    "\n",
    "load_and_normalize('bef', '../data/embeddings/orig_w2v', vocab, wv, w2i)\n",
    "load_and_normalize('aft', '../data/embeddings/hard_debiased_w2v', vocab, wv, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def topK(w, space, k=10):\n",
    "    \n",
    "    # extract the word vector for word w\n",
    "    idx = w2i[space][w]\n",
    "    vec = wv[space][idx, :]\n",
    "    \n",
    "    # compute similarity of w with all words in the vocabulary\n",
    "    sim = wv[space].dot(vec)\n",
    "    # sort similarities by descending order\n",
    "    sort_sim = (sim.argsort())[::-1]\n",
    "\n",
    "    # choose topK\n",
    "    best = sort_sim[:(k+1)]\n",
    "\n",
    "    return [vocab[space][i] for i in best if i!=idx]\n",
    "\n",
    "\n",
    "def similarity(w1, w2, space):\n",
    "    \n",
    "    i1 = w2i[space][w1]\n",
    "    i2 = w2i[space][w2]\n",
    "    vec1 = wv[space][i1, :]\n",
    "    vec2 = wv[space][i2, :]\n",
    "\n",
    "    return np.inner(vec1,vec2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "\n",
    "def has_punct(w):\n",
    "    \n",
    "    if any([c in string.punctuation for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_digit(w):\n",
    "    \n",
    "    if any([c in '0123456789' for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def limit_vocab(space, exclude = None):\n",
    "    vocab_limited = []\n",
    "    for w in tqdm(vocab[space][:50000]): \n",
    "        if w.lower() != w:\n",
    "            continue\n",
    "        if len(w) >= 20:\n",
    "            continue\n",
    "        if has_digit(w):\n",
    "            continue\n",
    "        if '_' in w:\n",
    "            p = [has_punct(subw) for subw in w.split('_')]\n",
    "            if not any(p):\n",
    "                vocab_limited.append(w)\n",
    "            continue\n",
    "        if has_punct(w):\n",
    "            continue\n",
    "        vocab_limited.append(w)\n",
    "    \n",
    "    if exclude:\n",
    "        vocab_limited = list(set(vocab_limited) - set(exclude))\n",
    "    \n",
    "    print \"size of vocabulary:\", len(vocab_limited)\n",
    "    \n",
    "    wv_limited = np.zeros((len(vocab_limited), 300))\n",
    "    for i,w in enumerate(vocab_limited):\n",
    "        wv_limited[i,:] = wv[space][w2i[space][w],:]\n",
    "    \n",
    "    w2i_limited = {w: i for i, w in enumerate(vocab_limited)}\n",
    "    \n",
    "    return vocab_limited, wv_limited, w2i_limited\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the reduced vocabularies and embeddings before and after, without gendered specific words\n",
    "\n",
    "import json\n",
    "with codecs.open('../data/lists/gender_specific_full.json') as f:\n",
    "    gender_specific = json.load(f)\n",
    "with codecs.open('../data/lists/definitional_pairs.json') as f:\n",
    "    definitional_pairs = json.load(f)\n",
    "with codecs.open('../data/lists/equalize_pairs.json') as f:\n",
    "    equalize_pairs = json.load(f)\n",
    "\n",
    "exclude_words = []\n",
    "for pair in definitional_pairs + equalize_pairs:\n",
    "    exclude_words.append(pair[0])\n",
    "    exclude_words.append(pair[1])\n",
    "\n",
    "exclude_words = list(set(exclude_words).union(set(gender_specific)))\n",
    "\n",
    "# create spaces of limited vocabulary\n",
    "vocab['limit_bef'], wv['limit_bef'], w2i['limit_bef'] = limit_vocab('bef', exclude = exclude_words)\n",
    "vocab['limit_aft'], wv['limit_aft'], w2i['limit_aft'] = limit_vocab('aft', exclude = exclude_words)\n",
    "\n",
    "assert(vocab['limit_aft'] == vocab['limit_bef'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute bias-by-projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the bias, before and after\n",
    "\n",
    "def compute_bias_by_projection(space_to_tag, full_space):\n",
    "    males = wv[space_to_tag].dot(wv[full_space][w2i[full_space]['he'],:])\n",
    "    females = wv[space_to_tag].dot(wv[full_space][w2i[full_space]['she'],:])\n",
    "    d = {}\n",
    "    for w,m,f in zip(vocab[space_to_tag], males, females):\n",
    "        d[w] = m-f\n",
    "    return d\n",
    "\n",
    "# compute bias-by-projection before and after debiasing\n",
    "gender_bias_bef = compute_bias_by_projection('limit_bef', 'bef')\n",
    "gender_bias_aft = compute_bias_by_projection('limit_aft', 'aft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the avg bias of the vocabulary (abs) before and after debiasing\n",
    "\n",
    "def report_bias(gender_bias):\n",
    "    bias = 0.0\n",
    "    for k in gender_bias:\n",
    "        bias += np.abs(gender_bias[k])\n",
    "    print bias/len(gender_bias)\n",
    "report_bias(gender_bias_bef)\n",
    "report_bias(gender_bias_aft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Coreelation between bias-by-projection and bias-by-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tuples of biases and counts of masculine/feminine NN for each word (for bias-by-neighbors)\n",
    "\n",
    "def bias_by_neighbors(space, neighbours_num = 100):\n",
    "    \n",
    "    tuples = []\n",
    "    for w in tqdm(vocab[space]):\n",
    "        \n",
    "        top = topK(w, space, k=neighbours_num+5)[:neighbours_num]\n",
    "\n",
    "        m = 0\n",
    "        f = 0    \n",
    "        for t in top:\n",
    "            if gender_bias_bef[t] > 0:\n",
    "                m+=1\n",
    "            else:\n",
    "                f+=1\n",
    "            \n",
    "        tuples.append((w, gender_bias_bef[w], gender_bias_aft[w], m, f))\n",
    "\n",
    "    return tuples\n",
    "        \n",
    "tuples_bef = bias_by_neighbors('limit_bef') \n",
    "tuples_aft = bias_by_neighbors('limit_aft')       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation between bias-by-projection and bias-by-neighbors\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "def pearson(a,b):\n",
    "   \n",
    "    return scipy.stats.pearsonr(a,b)\n",
    "\n",
    "def compute_corr(tuples, i1, i2):\n",
    "    \n",
    "    a = []\n",
    "    b = []\n",
    "    for t in tuples:\n",
    "        a.append(t[i1])\n",
    "        b.append(t[i2])\n",
    "    assert(len(a)==len(b))    \n",
    "    print pearson(a,b)\n",
    "\n",
    "compute_corr(tuples_bef, 1, 3)\n",
    "compute_corr(tuples_aft, 1, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Experiment - Visualize clusters of most biased words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary finctions\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "from cycler import cycler\n",
    "%matplotlib inline\n",
    "mpld3.enable_notebook()\n",
    "mpl.rc(\"savefig\", dpi=200)\n",
    "mpl.rcParams['figure.figsize'] = (8,8)\n",
    "mpl.rcParams['axes.prop_cycle'] = cycler(color='rc')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "#from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(vectors, words, labels, ax, title, random_state, num_clusters = 2):\n",
    "    \n",
    "    # perform TSNE\n",
    "    \n",
    "    X_embedded = TSNE(n_components=2, random_state=random_state).fit_transform(vectors)\n",
    "    if num_clusters == 2:\n",
    "        for x,l in zip(X_embedded, labels):\n",
    "            if l:\n",
    "                ax.scatter(x[0], x[1], marker = '.', c = 'c')\n",
    "            else:\n",
    "                ax.scatter(x[0], x[1], marker = 'x', c = 'darkviolet')\n",
    "    else:\n",
    "        ax.scatter(X_embedded[:,0], X_embedded[:,1], c = labels)                \n",
    "    \n",
    "    ax.text(.01, .9, title ,transform=ax.transAxes, fontsize=18)\n",
    "\n",
    "    \n",
    "def extract_vectors(words, space1 = 'limit_bef', space2 = 'limit_aft'):\n",
    "    \n",
    "    size = len(words)/2\n",
    "    \n",
    "    X_bef = [wv[space1][w2i[space1][x],:] for x in words]\n",
    "    X_aft = [wv[space2][w2i[space2][x],:] for x in words]\n",
    "\n",
    "    return X_bef, X_aft\n",
    "\n",
    "\n",
    "def cluster_and_visualize(words, X_bef, X_aft, random_state, y_true, num=2):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n",
    "    \n",
    "    y_pred_bef = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_bef)\n",
    "    visualize(X_bef, words, y_pred_bef, axs[0], 'Original', random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_bef) ]\n",
    "    print 'precision bef', sum(correct)/float(len(correct))\n",
    "    \n",
    "    y_pred_aft = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_aft)\n",
    "    visualize(X_aft, words, y_pred_aft, axs[1], 'Debiased', random_state)\n",
    "    correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_aft) ]\n",
    "    print 'precision aft', sum(correct)/float(len(correct))\n",
    "    fig.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster most biased words before and after debiasing\n",
    "import operator\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "size = 500\n",
    "sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "female = [item[0] for item in sorted_g[:size]]\n",
    "male = [item[0] for item in sorted_g[-size:]]\n",
    "\n",
    "X_bef, X_aft = extract_vectors(male + female)\n",
    "y_true = [1]*size + [0]*size\n",
    "cluster_and_visualize(male + female, X_bef, X_aft, random_state, y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professions experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_professions():\n",
    "    professions = []\n",
    "    with codecs.open('../data/lists/professions.json', 'r', 'utf-8') as f:\n",
    "        professions_data = json.load(f)\n",
    "    for item in professions_data:\n",
    "        professions.append(item[0].strip())\n",
    "    return professions\n",
    "\n",
    "\n",
    "professions = extract_professions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'professions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m         tuples\u001b[38;5;241m.\u001b[39mappend((w, gender_bias_bef[w], gender_bias_aft[w], m, f))\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tuples\n\u001b[0;32m---> 25\u001b[0m tuples_bef_prof \u001b[38;5;241m=\u001b[39m get_tuples_prof(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlimit_bef\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mprofessions\u001b[49m, gender_bias_bef)\n\u001b[1;32m     26\u001b[0m tuples_aft_prof \u001b[38;5;241m=\u001b[39m get_tuples_prof(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlimit_aft\u001b[39m\u001b[38;5;124m'\u001b[39m, professions, gender_bias_bef)\n\u001b[1;32m     28\u001b[0m compute_corr(tuples_bef_prof, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'professions' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def get_tuples_prof(space, words, gender_bias_dict):\n",
    "\n",
    "    tuples = []\n",
    "    for w in words:\n",
    "        if w not in gender_bias_dict:\n",
    "            continue\n",
    "            \n",
    "        top = topK(w, space, k=105)[:100]\n",
    "            \n",
    "        m = 0\n",
    "        f = 0  \n",
    "        for t in top:          \n",
    "            if gender_bias_dict[t] > 0:\n",
    "                m+=1\n",
    "            else:\n",
    "                f+=1\n",
    "                \n",
    "        tuples.append((w, gender_bias_bef[w], gender_bias_aft[w], m, f))\n",
    "        \n",
    "    return tuples\n",
    "\n",
    "\n",
    "tuples_bef_prof = get_tuples_prof('limit_bef', professions, gender_bias_bef)\n",
    "tuples_aft_prof = get_tuples_prof('limit_aft', professions, gender_bias_bef)\n",
    "\n",
    "compute_corr(tuples_bef_prof, 1, 3)\n",
    "compute_corr(tuples_aft_prof, 1, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots\n",
    "\n",
    "def show_plots(tuples_bef_prof, tuples_aft_prof):\n",
    "    \n",
    "    fig, axs = plt.subplots(2,1, figsize=(8,8))\n",
    "    \n",
    "    for i,(tuples, title) in enumerate(zip([tuples_bef_prof, tuples_aft_prof], ['Original', 'Debiased'])):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for t in tuples:\n",
    "            X.append(t[1])\n",
    "            Y.append(t[3])\n",
    "\n",
    "        axs[i].scatter(X,Y, color = 'c', s=12)\n",
    "        axs[i].set_ylim(0,100)\n",
    "        \n",
    "        for t in tuples:\n",
    "            if t[0] in ['nanny', 'librarian', 'hairdresser', 'receptionist', 'nurse',\\\n",
    "                       'consultant', 'warden', 'archaeologist', 'banker', 'comic',\\\n",
    "                        'warrior', 'skipper', 'captain', 'commander', 'coach']:\n",
    "                axs[i].annotate(t[0], xy=(t[1], t[3]), xytext=(t[1], t[3]), textcoords=\"data\", fontsize=12) \n",
    "        axs[i].text(.03, .85, title, transform=axs[i].transAxes, fontsize=20)\n",
    "    \n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "show_plots(tuples_bef_prof, tuples_aft_prof)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 5000 most biased words, split each polarity randomly to train (1/5) and test (4/5), and predict\n",
    "\n",
    "from sklearn import svm\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "\n",
    "\n",
    "def train_and_predict(space_train, space_test):\n",
    "    \n",
    "    X_train = [wv[space_train][w2i[space_train][w],:] for w in males[:size_train]+females[:size_train]]\n",
    "    Y_train = [1]*size_train + [0]*size_train\n",
    "    X_test = [wv[space_test][w2i[space_test][w],:] for w in males[size_train:]+females[size_train:]]\n",
    "    Y_test = [1]*size_test + [0]*size_test\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print 'train with', space_train\n",
    "    print 'test with', space_test\n",
    "\n",
    "    preds = clf.predict(X_test)\n",
    "\n",
    "    accuracy = [1 if y==z else 0 for y,z in zip(preds, Y_test)]\n",
    "    print 'accuracy:', float(sum(accuracy))/len(accuracy)\n",
    "\n",
    "    \n",
    "# extract nost biased words\n",
    "\n",
    "size_train = 500\n",
    "size_test = 2000\n",
    "size = size_train + size_test\n",
    "sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "females = [item[0] for item in sorted_g[:size]]\n",
    "males = [item[0] for item in sorted_g[-size:]]\n",
    "for f in females:\n",
    "    assert(gender_bias_bef[f] < 0)\n",
    "for m in males:\n",
    "    assert(gender_bias_bef[m] > 0)\n",
    "shuffle(females)\n",
    "shuffle(males)\n",
    "\n",
    "# classification before debiasing\n",
    "\n",
    "train_and_predict('bef', 'bef')\n",
    "\n",
    "# classification after debiasing\n",
    "\n",
    "train_and_predict('aft', 'aft')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Experiments (Calisken et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions for experiments by Caliskan et al.\n",
    "\n",
    "import scipy\n",
    "import scipy.misc as misc\n",
    "import itertools\n",
    "\n",
    "\n",
    "def s_word(w, A, B, space, all_s_words):\n",
    "    \n",
    "    if w in all_s_words:\n",
    "        return all_s_words[w]\n",
    "    \n",
    "    mean_a = []\n",
    "    mean_b = []\n",
    "    \n",
    "    for a in A:\n",
    "        mean_a.append(similarity(w, a, space))\n",
    "    for b in B:\n",
    "        mean_b.append(similarity(w, b, space))\n",
    "        \n",
    "    mean_a = sum(mean_a)/float(len(mean_a))\n",
    "    mean_b = sum(mean_b)/float(len(mean_b))\n",
    "    \n",
    "    all_s_words[w] = mean_a - mean_b\n",
    "\n",
    "    return all_s_words[w]\n",
    "\n",
    "\n",
    "def s_group(X, Y, A, B, space, all_s_words):\n",
    "    \n",
    "    total = 0\n",
    "    for x in X:\n",
    "        total += s_word(x, A, B, space, all_s_words)\n",
    "    for y in Y:\n",
    "        total -= s_word(y, A, B, space, all_s_words)\n",
    "        \n",
    "    return total\n",
    "\n",
    "\n",
    "def p_value_exhust(X, Y, A, B, space):\n",
    "    \n",
    "    if len(X) > 10:\n",
    "        print 'might take too long, use sampled version: p_value'\n",
    "        return\n",
    "    \n",
    "    assert(len(X) == len(Y))\n",
    "    \n",
    "    all_s_words = {}\n",
    "    s_orig = s_group(X, Y, A, B, space, all_s_words) \n",
    "    \n",
    "    union = set(X+Y)\n",
    "    subset_size = len(union)/2\n",
    "    \n",
    "    larger = 0\n",
    "    total = 0\n",
    "    for subset in tqdm(set(itertools.combinations(union, subset_size))):\n",
    "        total += 1\n",
    "        Xi = list(set(subset))\n",
    "        Yi = list(union - set(subset))\n",
    "        if s_group(Xi, Yi, A, B, space, all_s_words) > s_orig:\n",
    "            larger += 1\n",
    "    print 'num of samples', total\n",
    "    return larger/float(total)\n",
    "\n",
    "\n",
    "def p_value_sample(X, Y, A, B, space):\n",
    "    \n",
    "    random.seed(10)\n",
    "    np.random.seed(10)\n",
    "    all_s_words = {}\n",
    "    \n",
    "    assert(len(X) == len(Y))\n",
    "    length = len(X)\n",
    "    \n",
    "    s_orig = s_group(X, Y, A, B, space, all_s_words) \n",
    "    \n",
    "    num_of_samples = min(1000000, int(scipy.special.comb(length*2,length)*100))\n",
    "    print 'num of samples', num_of_samples\n",
    "    larger = 0\n",
    "    for i in range(num_of_samples):\n",
    "        permute = np.random.permutation(X+Y)\n",
    "        Xi = permute[:length]\n",
    "        Yi = permute[length:]\n",
    "        if s_group(Xi, Yi, A, B, space, all_s_words) > s_orig:\n",
    "            larger += 1\n",
    "    \n",
    "    return larger/float(num_of_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1\n",
    "\n",
    "A = ['John', 'Paul', 'Mike', 'Kevin', 'Steve', 'Greg', 'Jeff', 'Bill']\n",
    "B = ['Amy', 'Joan', 'Lisa', 'Sarah', 'Diana', 'Kate', 'Ann', 'Donna']\n",
    "C = ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career']\n",
    "D = ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives']\n",
    "\n",
    "print p_value_exhust(A, B, C, D, 'aft')\n",
    "\n",
    "# Experiment 2\n",
    "\n",
    "E = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition']\n",
    "F = ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n",
    "\n",
    "print p_value_exhust(A, B, E, F, 'aft')\n",
    "\n",
    "\n",
    "# Experiment 3\n",
    "\n",
    "G = ['science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment', 'astronomy']\n",
    "H = ['poetry', 'art', 'shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']\n",
    "\n",
    "print p_value_exhust(A, B, G, H, 'aft')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streaming",
   "language": "python",
   "name": "streaming"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
